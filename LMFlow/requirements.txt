packaging
numpy==1.24.2
datasets==2.14.6
tokenizers>=0.13.3
torch>=2.0.1
wandb==0.14.0
deepspeed==0.10.0
trl>=0.7.11
sentencepiece
transformers>=4.31.0
flask
flask_cors
icetk
cpm_kernels==1.0.11
evaluate==0.4.0
scikit-learn==1.2.2
dill<0.3.5
bitsandbytes>=0.40.0
pydantic<=1.10.9
gradio
accelerate>=0.27.2
einops>=0.6.1
scikit-learn==1.2.2
